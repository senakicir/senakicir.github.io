---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---

<div>
    <div class="split left">
        <div>
        <img id="pp" src="https://lh4.googleusercontent.com/Wb9vWlC-QU37fr3X-cTK6Y8BQoxRwpokjxCeu4heSb0mCMv-fwoLRQZrr-XoCA0C5Ag=w2400" />
        </div>
    </div>
    <div class="split right">
        <p>Hi there! I'm Sena, a 5th year PhD Candidate in EPFL <a href="http://cvlab.epfl.ch">Computer Vision Lab (CVLab)</a>, supervised by Prof. <a href="https://people.epfl.ch/pascal.fua/bio?lang=en">Pascal Fua</a> and Dr.<a href="https://people.epfl.ch/mathieu.salzmann"> Mathieu Salzmann.</a> My research interests are mainly in human motion analysis, more specifically, motion prediction and active human pose estimation. In the summer of 2021, I was on an internship with the Autonomous Systems group at Microsoft, where I worked on visual odometry.</p>
        <p>My undergraduate is from Bilkent University, Electrical and Electronics Engineering. I graduated in <a class="page-link" href="/bilkent">2017.</a></p>
        <p>
        <div id="bloglink"><h3><a class="page-link" href="/misc">Misc.</a></h3> </div>
        </p>
    </div>
</div>

<table class="invisible_table" >

<tr><th colspan="2">
<h2 class="spanswhole" style="text-align: center; margin-top:20px;">Publications:</h2>
</th></tr>

<tr><td  style="padding-right:50px">
    <img id="pub_p_small" src="https://lh3.googleusercontent.com/yWeppd2g7G1RRGeBA9r2Zh4eZSlKCWswUuow_S69jvsImTdwtLQtHXjhuHjikN8n-VA=w2400" />
    </td>
    <td>
        <p><h3>3D Pose Based Feedback For Physical Exercises</h3></p>
        <p><a href="https://arxiv.org/abs/2208.03257">[Pdf]</a><a href="https://github.com/Jacoo-Zhao/3D-Pose-Based-Feedback-For-Physical-Exercises">[Code]</a><a href="https://youtu.be/W3kyyeHe0SI">[Video]</a><a href="/projects/exercise_feedback">[Website]</a></p>
        <p>Ziyi Zhao, <b>Sena Kiciroglu</b>, Hugues Vinzant, Yuan Cheng, Isinsu Katircioglu, Mathieu Salzmann, Pascal Fua</p>
        <p><i>ACCV 2022</i></p>
        <p>Unsupervised physical training can cause serious injuries if performed incorrectly. We introduce a learning-based framework that identifies the mistakes made by a user and proposes corrective measures for easier and safer individual training.</p>
</td></tr>

<tr><td  style="padding-right:50px">
    <img id="pub_p_small" src="https://lh5.googleusercontent.com/mey6OuhGhJa2k-q9aKVRZa7OmUCTUWIBRBxY5dSj-pMCwp4wIfFFk-k5sJxcRgv-ne8=w2400" />
    </td>
    <td>
        <p><h3>Long Term Motion Prediction Using Keyposes</h3></p>
        <p><a href="https://arxiv.org/pdf/2012.04731.pdf">[Pdf]</a><a href="https://github.com/senakicir/keypose-prediction">[Code]</a><a href="https://youtu.be/hsHvKUm5GuU">[Video]</a><a href="/projects/keyposes">[Website]</a></p>
        <p><b>Sena Kiciroglu</b>, Wei Wang, Mathieu Salzmann, Pascal Fua</p>
        <p>3DV 2022 - <b>Oral</b></p>
        <p>We predict diverse and realistic long term (5 second) human motion by making use of "keyposes", the set of poses which we can use to accurately reconstruct the complete sequence.</p>
</td></tr>

<tr><td style="padding-right:50px">
        <img id="pub_p" src="https://lh6.googleusercontent.com/xjDrdnW2AfgaYBb2HQPNxV-R3xhQmiNGDfD20hd8zMPZzVpQ8QJG7lGt3_2lkeeDe48=w2400
" />
</td><td>
        <p><h3>Motion Prediction Using Temporal Inception Module</h3></p>
        <p><a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Lebailly_Motion_Prediction_Using_Temporal_Inception_Module_ACCV_2020_paper.pdf">[Pdf]</a><a href="https://github.com/tileb1/motion-prediction-tim">[Code]</a></p>
        <p>Tim Lebailly, <b>Sena Kiciroglu</b>, Mathieu Salzmann, Pascal Fua, Wei Wang</p>
        <p>ACCV 2020 </p>
        <p>We predict human motion accurately in the short and long term future by making use of a Temporal Inception Module (TIM). Using TIM, we produce input embeddings using convolutional layers, by using different kernel sizes for different input lengths.</p>
</td></tr>

<tr><td  style="border-bottom:0px;" >
        <img id="pub_p_small" src="https://lh6.googleusercontent.com/pUvF1Mls9SEsLMbDPmA2Fd998wrnJmpZv1qICZW9dgfG2tv4MZlan1VqbhIy5Yscm-w=w2400" />
</td><td style="border-bottom:0px;" >
        <p><h3>ActiveMoCap: Optimized Viewpoint Selection for Active Human Motion Capture</h3></p>
        <p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kiciroglu_ActiveMoCap_Optimized_Viewpoint_Selection_for_Active_Human_Motion_Capture_CVPR_2020_paper.pdf">[Pdf]</a><a href="https://github.com/senakicir/ActiveMoCap">[Code]</a><a href="https://youtu.be/Dqv7ZJQi28o">[Video]</a></p>
        <p><b>Sena Kiciroglu</b>, Helge Rhodin, Sudipta N. Sinha, Mathieu Salzmann, Pascal Fua</p>
        <p>CVPR 2020 - <b>Oral</b></p>
        <p>Given a short video sequence, we introduce an algorithm that predicts which viewpoints should be chosen by a moving camera to capture future frames so as to maximize 3D human pose estimation accuracy.</p>
</td></tr>


</table>


